{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HA4 by Adil Akhmetov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ir_measures import *\n",
    "import ir_measures\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import time\n",
    "from elasticsearch import Elasticsearch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "%config IPCompleter.greedy=True\n",
    "from time import time\n",
    "import csv\n",
    "import torch\n",
    "import pandas as pd\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Re-rank top20 documents returned for WikiIR test queries by Elasticsearch (or another retriever used in HA2)\n",
    "\n",
    "We already have top20 documents returned for WikiIR test queries by Elasticsearch from HA2. I only modified the results functions to add the content and query itself. Now, the `corpuses.csv` file contains the following columns:\n",
    "\n",
    "\t- queryId: the query id\n",
    "\t- docId: the document id\n",
    "\t- score: the score returned by Elasticsearch\n",
    "\t- query: the query itself\n",
    "\t- content: the content of the document\n",
    "\n",
    "The modified code to generate `corpuses.csv` file:\n",
    "```python\n",
    "def run_test_queries():\n",
    "    with open('../wikIR1k/test/queries.csv', 'r') as csvfile:\n",
    "        with open(\"corpuses.csv\", 'w') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['queryID', 'docID', 'score', 'query', 'content'])\n",
    "            reader = csv.reader(csvfile)\n",
    "            next(reader, None)\n",
    "            for row in reader:\n",
    "                query_id = int(row[0])\n",
    "                query = {\n",
    "                    'query': {\n",
    "                        'bool': {\n",
    "                            'should': {\n",
    "                                'match_phrase': {\n",
    "                                    'content.stemmed': row[1]\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "                res = es.search(index='wikir', body=query, size=20)['hits']\n",
    "                for hit in res['hits']:\n",
    "                    data = [query_id, hit['_id'],\n",
    "                            hit['_score'], row[1], hit['_source']['content']]\n",
    "                    writer.writerow(data)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Use cosine similarity between query and document embeddings to rank documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('msmarco-distilbert-dot-v5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank(query, queryId, corpus, writer, top = 20):\n",
    "\tquery_embedding = model.encode(query, convert_to_tensor=True)\n",
    "\tcorpus_embeddings = model.encode(\n",
    "\t\t[content for _, content in corpus], convert_to_tensor=True)\n",
    "\n",
    "\ttop_k = min(len(corpus), top)\n",
    "\n",
    "\tcos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "\ttop_results = torch.topk(cos_scores, k=top_k)\n",
    "\n",
    "\tc = 0\n",
    "\tfor score, idx in zip(top_results[0], top_results[1]):\n",
    "\t\twriter.writerow([queryId, 0, corpus[idx][0], c, score.item(), 'CosSim'])\n",
    "\t\tc += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../seminar_elastic/corpuses.csv', 'r') as corpuses:\n",
    "    with open('corpuses.run', 'w') as runs:\n",
    "        reader = csv.reader(corpuses)\n",
    "        writer = csv.writer(runs, delimiter=' ')\n",
    "        next(reader)\n",
    "        corpus = []\n",
    "        query = None\n",
    "        queryId = None\n",
    "\n",
    "        while (row := next(reader, None)) is not None:\n",
    "            if queryId is None:\n",
    "                        queryId = row[0]\n",
    "                        query = row[3]\n",
    "\n",
    "            if row[0] != queryId:\n",
    "                rerank(query, queryId, corpus, writer)\n",
    "\n",
    "                queryId = row[0]\n",
    "                query = row[3]\n",
    "                corpus = []\n",
    "            corpus.append((row[1], row[4]))\n",
    "        \n",
    "        rerank(query, queryId, corpus, writer)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Evaluate new rankings using P@10, p@20, MAP@20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{P@10: 0.174, AP: 0.11989138021776306, P@20: 0.10949999999999999}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels = ir_measures.read_trec_qrels('../wikIR1k/test/qrels')\n",
    "run = ir_measures.read_trec_run('./corpuses.run')\n",
    "ir_measures.calc_aggregate([P@10, P@20, MAP], qrels, run)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task*\n",
    "Re-rank documents based on a combination of BM25 scores and cosine\n",
    "similarity of query/document embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Sample 100-500 queries from train subset\n",
    "- ### Get top50 documents for each query using Elasticsearch (BM25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(\n",
    "    \"https://localhost:9200\",\n",
    "    verify_certs=False,\n",
    "    ssl_show_warn=False,\n",
    "    basic_auth=(\"elastic\", \"gdJplaadFKhdJuBysJDX\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = []\n",
    "SCORES = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_queries():\n",
    "    with open('../wikIR1k/training/queries.csv', 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader, None)\n",
    "        line = 0\n",
    "        for row in reader:\n",
    "            if line > 100:\n",
    "                break\n",
    "            line += 1\n",
    "\n",
    "            query_id = int(row[0])\n",
    "            query = {\n",
    "                'query': {\n",
    "                    'bool': {\n",
    "                        'should': {\n",
    "                            'match_phrase': {\n",
    "                                'content.stemmed': row[1]\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            res = es.search(index='wikir', body=query, size=50)['hits']\n",
    "\n",
    "            scores = [hit['_score'] for hit in res['hits']]\n",
    "            SCORES.extend(scores)\n",
    "\n",
    "            for hit in res['hits']:\n",
    "                data = [query_id, hit['_id'],\n",
    "                        hit['_score'], row[1], hit['_source']['content']]\n",
    "                runs.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k6/3zkmms5n2j3_c0wt5q78hm040000gn/T/ipykernel_54418/2159404077.py:23: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  res = es.search(index='wikir', body=query, size=50)['hits']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent 2.32 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "run_training_queries()\n",
    "print(f\"Time spent {time() - start:0.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Min-max normalize BM25 scores, so they are in the range [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in runs:\n",
    "    min_max = (run[2] - min(SCORES)) / (max(SCORES) - min(SCORES))\n",
    "    run.append(min_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[123839,\n",
       " '806300',\n",
       " 16.740032,\n",
       " 'yanni',\n",
       " 'it is a compilation of tracks from his five previous studio albums released between 1980 and 1989 plus three new compositions yanni was encouraged to release the album by his then partner actress linda evans reflections of passion became yanni s fastest selling and most successful album of his career upon release it reached no 1 on the billboard top new age albums chart and no 29 on the billboard 200 yanni supported the album with a nationwide concert tour in 1990 that featured his band and an orchestra in 1995 it was certified double platinum for selling 2 million copies in the us in august 1989 yanni released his fifth studio album niki nana the album marked his stylistic development from solo keyboard music towards rock with the addition of additional vocalists musicians and choir around the same time of its release yanni s newfound relationship with american actress linda evans who had become a fan of his music received press attention not long into their relationship evans pitched an idea she had for an album from yanni which was long in duration reflected a single mood throughout and something that she could play at dinner parties she mentioned',\n",
       " 0.37955932350949895]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Get cosines for query/document embeddings\n",
    "- ### Find an alpha that maximizes MAP@20 on train data `alpha*BM25 + (1-alpha)*q_d_cosine_similarity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_v2(query, queryId, corpus, writer, alpha = 0.1):\n",
    "\tquery_embedding = model.encode(query, convert_to_tensor=True)\n",
    "\tcorpus_embeddings = model.encode(\n",
    "\t\t[content for _, content, _ in corpus], convert_to_tensor=True)\n",
    "\n",
    "\ttop_k = min(len(corpus), 50)\n",
    "\n",
    "\tcos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "\ttop_results = torch.topk(cos_scores, k=top_k)\n",
    "\n",
    "\tc = 0\n",
    "\tfor score, idx in zip(top_results[0], top_results[1]):\n",
    "\t\tscore = alpha * corpus[idx][2] + (1 - alpha) * score.item()\n",
    "\t\twriter.writerow([queryId, 0, corpus[idx][0], c, score, 'CosSimBM25'])\n",
    "\t\tc += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMap20(alpha): \n",
    "\twith open(f'{alpha}_runs.csv', 'w') as csvfile:\n",
    "\t\twriter = csv.writer(csvfile, delimiter=' ')\n",
    "\t\t\n",
    "\t\tcorpus = []\n",
    "\t\tquery = None\n",
    "\t\tqueryId = None\n",
    "\n",
    "\t\tfor run in runs:\n",
    "\t\t\tif queryId is None:\n",
    "\t\t\t\tqueryId = run[0]\n",
    "\t\t\t\tquery = run[3]\n",
    "\n",
    "\t\t\tif run[0] != queryId:\n",
    "\t\t\t\trerank_v2(query, queryId, corpus, writer, alpha)\n",
    "\n",
    "\t\t\t\tqueryId = run[0]\n",
    "\t\t\t\tquery = run[3]\n",
    "\t\t\t\tcorpus = []\n",
    "\t\t\tcorpus.append((run[1], run[4], run[5]))\n",
    "\n",
    "\t\trerank_v2(query, queryId, corpus, writer, alpha)\n",
    "\n",
    "\tqrels = ir_measures.read_trec_qrels('../wikIR1k/training/qrels')\n",
    "\treturn ir_measures.calc_aggregate([MAP@20], qrels, ir_measures.read_trec_run(f'{alpha}_runs.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "map01 = getMap20(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "map03 = getMap20(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "map05 = getMap20(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "map08 = getMap20(0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{AP@20: 0.012652878728836033} map01\n",
      "{AP@20: 0.012514469132369439} map03\n",
      "{AP@20: 0.012203960977199892} map05\n",
      "{AP@20: 0.011710759272232175} map08\n"
     ]
    }
   ],
   "source": [
    "print(map01, 'map01')\n",
    "print(map03, 'map03')\n",
    "print(map05, 'map05')\n",
    "print(map08, 'map08')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Apply the formula to the test data (again, to top50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_queries():\n",
    "    with open('../wikIR1k/test/queries.csv', 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader, None)\n",
    "        line = 0\n",
    "        for row in reader:\n",
    "            if line > 100:\n",
    "                break\n",
    "            line += 1\n",
    "\n",
    "            query_id = int(row[0])\n",
    "            query = {\n",
    "                'query': {\n",
    "                    'bool': {\n",
    "                        'should': {\n",
    "                            'match_phrase': {\n",
    "                                'content.stemmed': row[1]\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            res = es.search(index='wikir', body=query, size=50)['hits']\n",
    "\n",
    "            scores = [hit['_score'] for hit in res['hits']]\n",
    "            SCORES.extend(scores)\n",
    "\n",
    "            for hit in res['hits']:\n",
    "                data = [query_id, hit['_id'],\n",
    "                        hit['_score'], row[1], hit['_source']['content']]\n",
    "                runs.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = []\n",
    "SCORES = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k6/3zkmms5n2j3_c0wt5q78hm040000gn/T/ipykernel_54418/816007642.py:23: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  res = es.search(index='wikir', body=query, size=50)['hits']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent 2.09 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "run_test_queries()\n",
    "print(f\"Time spent {time() - start:0.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in runs:\n",
    "    min_max = (run[2] - min(SCORES)) / (max(SCORES) - min(SCORES))\n",
    "    run.append(min_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'test_runs.csv', 'w') as csvfile:\n",
    "\t\twriter = csv.writer(csvfile, delimiter=' ')\n",
    "\n",
    "\t\tcorpus = []\n",
    "\t\tquery = None\n",
    "\t\tqueryId = None\n",
    "\n",
    "\t\tfor run in runs:\n",
    "\t\t\tif queryId is None:\n",
    "\t\t\t\tqueryId = run[0]\n",
    "\t\t\t\tquery = run[3]\n",
    "\n",
    "\t\t\tif run[0] != queryId:\n",
    "\t\t\t\trerank_v2(query, queryId, corpus, writer, 0.1)\n",
    "\n",
    "\t\t\t\tqueryId = run[0]\n",
    "\t\t\t\tquery = run[3]\n",
    "\t\t\t\tcorpus = []\n",
    "\t\t\tcorpus.append((run[1], run[4], run[5]))\n",
    "\n",
    "\t\trerank_v2(query, queryId, corpus, writer, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{P@10: 0.176, AP: 0.13685233493777194, P@20: 0.12200000000000003}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels = ir_measures.read_trec_qrels('../wikIR1k/test/qrels')\n",
    "run = ir_measures.read_trec_run('./test_runs.csv')\n",
    "ir_measures.calc_aggregate([P@10, P@20, MAP], qrels, run)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
