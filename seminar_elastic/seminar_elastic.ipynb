{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T11:18:30.190816Z",
     "start_time": "2019-09-19T11:18:27.791936Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import requests\n",
    "from time import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-19T11:16:53.333642Z",
     "start_time": "2019-09-19T11:16:53.216004Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "es = Elasticsearch(\n",
    "    \"https://localhost:9200\",\n",
    "    ca_certs=\"./http_ca.crt\",\n",
    "    basic_auth=(\"elastic\", \"RikVUty_*6bSkVMxGifL\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Create index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Create empty index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T20:56:48.324068Z",
     "start_time": "2019-09-18T20:56:47.639244Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "es.indices.create(index='wikir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Create configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can describe documents structure and specify the way it must be processed using `mappings`.\n",
    "\n",
    "Under `mapping.properties` we define fields of our index. For each field we have to specify its type. Common types are:\n",
    "\n",
    "- **text**: general purpose string type\n",
    "- **numeric** family\n",
    "- **date**\n",
    "- **boolean**\n",
    "- **keyword**: allows only full-match search\n",
    "- **object**/**nested**: stores complex JSON objects. **nested** support related field search, while **object** merges objects array field by field\n",
    "- **rank_feature**: keeps a number which is used when computing relevance\n",
    "\n",
    "Note that any type of field support arrays of this type out-of-the-box. The reason is that search engine has to deal with sequential data anyway, so array support could be implemented for free via using existing methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T20:56:53.413448Z",
     "start_time": "2019-09-18T20:56:53.405051Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "settings_1 = {\n",
    "    'mappings': {\n",
    "        'properties': {\n",
    "            'content': {\n",
    "                'type': 'text'\n",
    "            },\n",
    "            'doc_id': {\n",
    "                'type': 'numeric'\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Let us use analysis!\n",
    "\n",
    "What is analyzer?\n",
    "\n",
    "Search engine actually indexes a sequence of tokens. Analyzer is a module which takes raw content and returns resulting tokens depending on the our goal in every single case. These are the tokens which will be the keys in inverted index.\n",
    "\n",
    "Analyzer's pipeline looks like\n",
    "\n",
    " \\*content to be indexed\\* ->  \n",
    " ***0 or more*** *character filters* ->  \n",
    " ***exactly 1*** *tokenizer* ->  \n",
    " ***0 or more*** *token filters* ->  \n",
    " \\*actually indexing\\*\n",
    "\n",
    "1. *Character filter* edits characters, lol. The only common one I know is an HTML stripper.\n",
    "2. *Tokenizer* splits text to tokens, surprisingly.\n",
    "    - **standard**: some tricky algorithm, you should really be careful using with Russian\n",
    "    - **letter**: performing split each time it sees non-letter symbol\n",
    "    - **whitespace**: your choice if tokenization has been done\n",
    "    - **pattern**: splits by separator defined as Java regex\n",
    "    - **other**\n",
    "3. *Token filter* changes token stream, usually only changes or remove tokens. Therefore, it sometimes adds tokens, for example, in case of query expansion.\n",
    "    - **lowercase**\n",
    "    - **shingle**\n",
    "    - **stop**: removes stopwords, there is built-in support for many languages (not quite reliable, though)\n",
    "    - **stemmer** and **snowball**: for russian they are both snowball, but I'm not really sure if they are indentical\n",
    "    - **hunspell**: provides morphological analysis, but it is very limited due to dictionary-based algorithm\n",
    "    \n",
    "Also, we have some ready-to-use analyzers:  \n",
    "- **standard**: default analyzer, standard tokenizer + lowercase\n",
    "- **simple**: letter tokenizer + lowercase\n",
    "- **whitespace**: plain whitespace tokenizer\n",
    "- **keyword**: fairly does nothing\n",
    "- **pattern**: pattern tokenizer with optional lowercase\n",
    "- **language**: language-specific pipeline, but definitely not a silver bullet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### Set it up!\n",
    "\n",
    "Now what if we want to build a custom analyzer. We can do that using `setting.analysis` parameter.\n",
    "\n",
    "Suppose we've already done with morphological analysis, so we don't want to use any character filter and the best tokenizer is plain whitespace with limiting possible token length to 20. But, we still need lowercasing. Let's do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T20:57:02.163534Z",
     "start_time": "2019-09-18T20:57:02.157060Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "settings_3 = {\n",
    "    'mappings': {\n",
    "        'properties': {\n",
    "            'content': {\n",
    "                'type': 'text'\n",
    "            },\n",
    "            'doc_id': {\n",
    "                'type': 'numeric'\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    'settings': {\n",
    "        'analysis': {\n",
    "            'analyzer': {\n",
    "                'white_lover': {\n",
    "                    'tokenizer': 'white_20',\n",
    "                    'filter': [\n",
    "                        'lowercase'\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            'tokenizer': {\n",
    "                'white_20': {\n",
    "                    'type': 'whitespace',\n",
    "                    'max_token_length': 5\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now let's consider more complex setting. Here's what we want this time:\n",
    "\n",
    "1. Split text to tokens which consist of 2 or more word characters.\n",
    "2. Lowercase everything.\n",
    "3. Remove some set of stopwords.\n",
    "4. Apply stemming with english snowball.\n",
    "\n",
    "Well, this will require a bit of work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T20:57:05.354769Z",
     "start_time": "2019-09-18T20:57:05.346056Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "settings_4 = {\n",
    "    'mappings': {\n",
    "        'properties': {\n",
    "            'content': {\n",
    "                'type': 'text'\n",
    "            },\n",
    "            'doc_id': {\n",
    "                'type': 'numeric'\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    'settings': {\n",
    "        'analysis': {\n",
    "            'tokenizer': {\n",
    "                'word_longer_2': {\n",
    "                    'type': 'pattern',\n",
    "                    'pattern': '\\w{2,}',\n",
    "                    'group': 0\n",
    "                }\n",
    "            },\n",
    "            'filter': {\n",
    "                'au_stop': {\n",
    "                    'type': 'stop',\n",
    "                    'stopwords': stopwords\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now we want to use created custom analyzers to process certain document fields. Note if we assign an analyzer to a field, it will be applied to document content at index-time and also to query text at query-time (if query is asked to the same field). \n",
    "\n",
    "Also sometimes we want to handle the same field with different analyzers. We have to used `fields` property for that purpose, setting one analyzer per-subfield.\n",
    "\n",
    "Let's analyze `content` field with *white_lover* analyzer and `content` field with default analyzer, creating two subfields `white` and `complex` with *white_lover* and *russian_complex* analyzers respectively. We will be able to refer these fields in queries as `content.white` and `content.complex`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T20:57:09.807741Z",
     "start_time": "2019-09-18T20:57:09.793869Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "settings_final = {\n",
    "    'mappings': {\n",
    "        'properties': {\n",
    "            'content': {\n",
    "                'type': 'text',\n",
    "                'fields': {\n",
    "                    'white': {\n",
    "                        'type': 'text',\n",
    "                        'analyzer': 'white_lover'\n",
    "                    },\n",
    "                }\n",
    "            },\n",
    "            'doc_id': {\n",
    "                'type': 'integer'\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'settings': {\n",
    "        'analysis': {\n",
    "            'tokenizer': {\n",
    "                'word_longer_2': {\n",
    "                    'type': 'pattern',\n",
    "                    'pattern': '\\w{2,}',\n",
    "                    'group': 0\n",
    "                },\n",
    "                'white_20': {\n",
    "                    'type': 'whitespace',\n",
    "                    'max_token_length': 5\n",
    "                }\n",
    "            },\n",
    "            'filter': {\n",
    "                'english_snow': {\n",
    "                    'type': 'snowball',\n",
    "                    'language': 'english'\n",
    "                },\n",
    "            },\n",
    "            'analyzer': {\n",
    "                'white_lover': {\n",
    "                    'tokenizer': 'white_20',\n",
    "                },\n",
    "                'stemmer': {\n",
    "                    'tokenizer': 'word_longer_2',\n",
    "                    'filter': [\n",
    "                        'english_snow'\n",
    "                    ]\n",
    "                },\n",
    "            },\n",
    "        \n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Create index with proper configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to use index setting. Let's define a function which allows us to easily update index settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T20:57:17.421395Z",
     "start_time": "2019-09-18T20:57:17.415931Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def recreate_index(with_stemming=False):\n",
    "    es.indices.delete(index='wikir')\n",
    "    if with_stemming:\n",
    "        settings_final['mappings']['properties']['content']['fields']['stemmed'] = {\n",
    "            'type': 'text',\n",
    "            'analyzer': 'stemmer'\n",
    "        }\n",
    "    es.indices.create(index='wikir', body=settings_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T20:57:18.835802Z",
     "start_time": "2019-09-18T20:57:18.165906Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k6/3zkmms5n2j3_c0wt5q78hm040000gn/T/ipykernel_46833/294914630.py:8: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  es.indices.create(index='wikir', body=settings_final)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent without stemming 1.88 seconds\n",
      "Time spent with stemming 0.15 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "recreate_index(with_stemming=False)\n",
    "print(f\"Time spent without stemming {time() - start:0.2f} seconds\")\n",
    "\n",
    "start = time()\n",
    "recreate_index(with_stemming=True)\n",
    "print(f\"Time spent with stemming {time() - start:0.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Check custom analyzers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "After we set up analysis we want to check if it works in proper way. We can do this using Elastic `analyze` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T20:57:21.189403Z",
     "start_time": "2019-09-18T20:57:21.182496Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def check_analyzer(analyzer, text):\n",
    "    body = analyzer\n",
    "    body['text'] = text\n",
    "    \n",
    "    tokens = es.indices.analyze(index='wikir', body=body)['tokens']\n",
    "    tokens = [token_info['token'] for token_info in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T20:57:38.298608Z",
     "start_time": "2019-09-18T20:57:38.294633Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "text = 'a barcode is a machine readable optical label that contains information about the item'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We have to describe analyzer in any way and send it to `check_analyzer`. Let's check **standard** analyzer first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T20:58:00.985128Z",
     "start_time": "2019-09-18T20:58:00.943917Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k6/3zkmms5n2j3_c0wt5q78hm040000gn/T/ipykernel_46833/1309655745.py:5: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  tokens = es.indices.analyze(index='wikir', body=body)['tokens']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'barcode',\n",
       " 'is',\n",
       " 'a',\n",
       " 'machine',\n",
       " 'readable',\n",
       " 'optical',\n",
       " 'label',\n",
       " 'that',\n",
       " 'contains',\n",
       " 'information',\n",
       " 'about',\n",
       " 'the',\n",
       " 'item']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer = {\n",
    "    'analyzer': 'standard'\n",
    "}\n",
    "\n",
    "check_analyzer(analyzer, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "And now check **whitespace**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T20:58:05.276066Z",
     "start_time": "2019-09-18T20:58:05.236992Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k6/3zkmms5n2j3_c0wt5q78hm040000gn/T/ipykernel_46833/1309655745.py:5: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  tokens = es.indices.analyze(index='wikir', body=body)['tokens']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'barcode',\n",
       " 'is',\n",
       " 'a',\n",
       " 'machine',\n",
       " 'readable',\n",
       " 'optical',\n",
       " 'label',\n",
       " 'that',\n",
       " 'contains',\n",
       " 'information',\n",
       " 'about',\n",
       " 'the',\n",
       " 'item']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer = {\n",
    "    'analyzer': 'whitespace'\n",
    "}\n",
    "\n",
    "check_analyzer(analyzer, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Note that if we apply `analyze` query to some index, we can use analyzers, tokenizers and filters described inside it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T20:58:20.587721Z",
     "start_time": "2019-09-18T20:58:20.568091Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k6/3zkmms5n2j3_c0wt5q78hm040000gn/T/ipykernel_46833/1309655745.py:5: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  tokens = es.indices.analyze(index='wikir', body=body)['tokens']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'barco',\n",
       " 'de',\n",
       " 'is',\n",
       " 'a',\n",
       " 'machi',\n",
       " 'ne',\n",
       " 'reada',\n",
       " 'ble',\n",
       " 'optic',\n",
       " 'al',\n",
       " 'label',\n",
       " 'that',\n",
       " 'conta',\n",
       " 'ins',\n",
       " 'infor',\n",
       " 'matio',\n",
       " 'n',\n",
       " 'about',\n",
       " 'the',\n",
       " 'item']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer = {\n",
    "    'analyzer': 'white_lover'\n",
    "}\n",
    "\n",
    "check_analyzer(analyzer, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We got results, although **white_lover** is not built-in."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Try with **english**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T20:58:25.065215Z",
     "start_time": "2019-09-18T20:58:25.053545Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k6/3zkmms5n2j3_c0wt5q78hm040000gn/T/ipykernel_46833/1309655745.py:5: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  tokens = es.indices.analyze(index='wikir', body=body)['tokens']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['barcod',\n",
       " 'is',\n",
       " 'machin',\n",
       " 'readabl',\n",
       " 'optic',\n",
       " 'label',\n",
       " 'that',\n",
       " 'contain',\n",
       " 'inform',\n",
       " 'about',\n",
       " 'the',\n",
       " 'item']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer = {\n",
    "    'analyzer': 'stemmer'\n",
    "}\n",
    "\n",
    "check_analyzer(analyzer, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Well done, it is all good now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Index documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "At this point we want to add documents to the index. The easiest way to do this is using `parallel_bulk` API. First of all, we have to create a function, which builds an Elastic *action*. *Action* is actually just an index entry, which consist of several meta-fields. We will be focused on 3 of them. `_id` field is literally unique document identificator. `_index` field shows which index the document belongs to. And `_source` field contains document data itself as a JSON object. Let's code it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T20:59:14.288392Z",
     "start_time": "2019-09-18T20:59:14.283043Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def create_es_action(index, doc_id, document):\n",
    "    return {\n",
    "        '_index': index,\n",
    "        '_id': doc_id,\n",
    "        '_source': document\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now we have to get some iterable of actions. The most appropriate solution in many cases is creating a generator function. I have my data JSON-represented, so generator will be quite simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T20:59:15.550824Z",
     "start_time": "2019-09-18T20:59:15.545246Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def es_actions_generator():\n",
    "    with open('./wikIR1k/documents.csv', 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        next(reader, None)\n",
    "        for row in reader:\n",
    "            doc_id = int(row[0])\n",
    "            doc = {\n",
    "                'content': row[1],\n",
    "                'doc_id': doc_id,\n",
    "            }\n",
    "            yield create_es_action('wikir', doc_id, doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "And finally we run indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T20:59:17.571162Z",
     "start_time": "2019-09-18T20:59:17.243072Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent without stemming 51.71 seconds\n"
     ]
    }
   ],
   "source": [
    "# without stemming\n",
    "start = time()\n",
    "for ok, result in parallel_bulk(es, es_actions_generator(), queue_size=4, thread_count=4, chunk_size=1000):\n",
    "    if not ok:\n",
    "        print(result)\n",
    "print(f\"Time spent without stemming {time() - start:0.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent with stemming 56.60 seconds\n"
     ]
    }
   ],
   "source": [
    "# with stemming\n",
    "start = time()\n",
    "for ok, result in parallel_bulk(es, es_actions_generator(), queue_size=4, thread_count=4, chunk_size=1000):\n",
    "    if not ok:\n",
    "        print(result)\n",
    "print(f\"Time spent with stemming {time() - start:0.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here we are, ready to perform search!\n",
    "\n",
    "We will use `search` API, which takes query as a JSON object and returns a responce as a JSON object too. Let's define a pair of useful functions for visualization of results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run test queries, get top20 results for each query, estimate query execution time\n",
    "- Save triples `<queryID, docID, score>` for two variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_queries(filename):\n",
    "    with open('./wikIR1k/test/queries.csv', 'r') as csvfile:\n",
    "        with open(f\"{filename}.csv\", 'w') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['queryID', 'docID', 'score'])\n",
    "            reader = csv.reader(csvfile)\n",
    "            next(reader, None)\n",
    "            for row in reader:\n",
    "                query_id = int(row[0])\n",
    "                query = {\n",
    "                    'query': {\n",
    "                        'bool': {\n",
    "                            'should': {\n",
    "                                'match_phrase': {\n",
    "                                    'content': row[1]\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "                res = es.search(index='wikir', body=query, size=20)['hits']\n",
    "                for hit in res['hits']:\n",
    "                        data = [query_id, hit['_id'], hit['_score']]\n",
    "                        writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k6/3zkmms5n2j3_c0wt5q78hm040000gn/T/ipykernel_46833/1766794919.py:21: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  res = es.search(index='wikir', body=query, size=20)['hits']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent without stemming 1.14 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "run_test_queries(\"variant1\")\n",
    "print(f\"Time spent without stemming {time() - start:0.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k6/3zkmms5n2j3_c0wt5q78hm040000gn/T/ipykernel_46833/2765061889.py:21: DeprecationWarning: The 'body' parameter is deprecated and will be removed in a future version. Instead use individual parameters.\n",
      "  res = es.search(index='wikir', body=query, size=20)['hits']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent with stemming 1.19 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "run_test_queries(\"variant2\")\n",
    "print(f\"Time spent with stemming {time() - start:0.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider search engine $E$ gives documents sequence $(d_1, .., d_n)$ by query $q$. Then define $E(q) = (d_1, .., d_n)$.\n",
    "\n",
    "We use binary relevance model where relevance function $rel(q, d)$ returns 0 or 1.\n",
    "\n",
    "Let $k$ be positive number. Define number of relevant documents in the first k $s_k(q) = \\sum\\limits_1^k rel(q, E(q)_k)$. Define total number of relevant documents by query $q$ as $R(q) = \\sum\\limits_{d \\in collection} rel(q, d)$\n",
    "\n",
    "Also define *precision at level k*  $p_{@k}(q) = \\frac{s_k(q)}{k}$ and *recall at level k* $r_{@k}(q) = \\frac{s_k(q)}{R(q)}$.\n",
    "\n",
    "If we let $k = R(q)$ then we get $p_{@k}(q) = r_{@k}(q)$. This is called *R-precision*.\n",
    "\n",
    "Since now we didn't pay attention to ranking. We want to give higher weights to relevant documents at top and lower weights to documents at bottom. We will aggregate $p_{@i}$ by $i$ from $1$ to $k$ and it leads us to *Mean Average Precision*.\n",
    "\n",
    "$MAP_{@k} = \\frac{1}{k} \\sum\\limits_1^k rel(q, E(q)_k)$\n",
    "\n",
    "In fact, we usually evaluate a system with a bunch of queries, so the formulas will be:\n",
    "\n",
    "$p_{@k}(q) = \\frac{1}{|Q|} \\sum\\limits_{q \\in Q} \\frac{s_k(q)}{k}$\n",
    "\n",
    "$r_{@k}(q) = \\frac{1}{|Q|} \\sum\\limits_{q \\in Q} \\frac{s_k(q)}{R(q)}$\n",
    "\n",
    "$MAP_{@k} = \\frac{1}{|Q|} \\sum\\limits_{q \\in Q} \\frac{1}{k} \\sum\\limits_1^k rel(q, E(q)_k)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_measures\n",
    "from ir_measures import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trec(filename):\n",
    "    with open(f\"{filename}.csv\", 'r') as csvfile:\n",
    "        with open(f\"{filename}.qrels.csv\", 'w') as qrels_file:\n",
    "            with open(f\"{filename}.run.csv\", 'w') as run_file:\n",
    "                reader = csv.reader(csvfile)\n",
    "                qrels_writer = csv.writer(qrels_file, delimiter=' ')\n",
    "                run_writer = csv.writer(run_file, delimiter=' ')\n",
    "\n",
    "                next(reader, None)\n",
    "\n",
    "                i = 0\n",
    "                queryId = 0\n",
    "                j = 0\n",
    "                for row in reader:\n",
    "                    qrels_writer.writerow(\n",
    "                        [row[0], i, row[1], int(row[0] != row[1])])\n",
    "\n",
    "                    if row[0] != queryId:\n",
    "                        queryId = row[0]\n",
    "                        j = 0\n",
    "\n",
    "                    run_writer.writerow([row[0], 0, row[1], j, row[2], 'BM25'])\n",
    "                    i += 1\n",
    "                    j += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_trec(\"variant1\")\n",
    "generate_trec(\"variant2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{AP: 0.9319861300671777, P@10: 0.8353535353535356, P@20: 0.7712121212121219}"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels = ir_measures.read_trec_qrels('./variant1.qrels.csv')\n",
    "run = ir_measures.read_trec_run('./variant1.run.csv')\n",
    "ir_measures.calc_aggregate([P@10, P@20, MAP], qrels, run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{AP: 0.9308992979521602, P@10: 0.8434343434343435, P@20: 0.7838383838383847}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels = ir_measures.read_trec_qrels('./variant2.qrels.csv')\n",
    "run = ir_measures.read_trec_run('./variant2.run.csv')\n",
    "ir_measures.calc_aggregate([P@10, P@20, MAP], qrels, run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{AP: 0.11196168401599797, P@10: 0.1319999999999999, P@20: 0.09499999999999999}"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels = ir_measures.read_trec_qrels('./wikIR1k/test/qrels')\n",
    "run = ir_measures.read_trec_run('./wikIR1k/test/BM25.res')\n",
    "ir_measures.calc_aggregate([P@10, P@20, MAP], qrels, run)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
